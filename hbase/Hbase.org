* Hbase
** 概念
1. 什么是Hbase?
   1. Google Bigtable的开源实现
   2. 分布式数据库
   3. 列式存储
   4. NoSql
2. 列式存储的优势
   1. 行中没有保存数据的列不占空间
3. Hbase为什么高效？
   1. 将随机读写转换为顺序读写，适合高并发写入
   2. 均衡效果好，读写性能与机器数线性相关
   3. 空间高效利用，行中没有保存数据的列不占空间，Hbase存储更为紧凑
** 设计
*** HFile (hbase-3857)
**** motivation
1. v1下，rs中由于过大的bloom过滤器和block indexes，而引起的高内存占用和拖慢rs启动时间
2. 只有当region的block index data加载完成，这个region才被视为open
3. bloom fileter的情况是：第一次使用bloom filter的get请求会导致整个bloom filter的加载，相当于第一个get请求被放大了
4. 高内存占用,可以把部分内容写出到HFile.加载时候可以部分加载.为了达到这样的效果Bloom Filter和 block indexes都被拆分成多个
5. 对于每个拆分出来的Bloom filter,当收集了足够的key,能够高效的利用bit 数组时,就可以write out
6. 对于每个拆分出来的Block index,收集到预期的一个值后,就可以write out
7. 在hfile中bloom filter和index block被成为inline block
8. 在v1中datablock是紧邻在一起的,所以可以用下一个block的offset来确定上一个block的length.加入inlineblock后,失去了这个方便
9. 另一个改动,也就是多级索引,是为了加快rs的启动
**** v2 格式
***** byte type
8byte 的字节序列,等于v1的magic records
1. DATA 数据block
2. LEAF_INDEX 多级索引的页索引
3. BLOOM_CHUNK bloom filter的块
4. META 元数据块 在v2中不在用于bloom filter
5. INTERMEDIATE_INDEX 多级索引的中间索引
6. ROOT_INDEX 多级索引的根索引
7. FILE_INFO 元数据的kv map
8. BLOOM_META 在load_on_open分区中的Bloom过滤器的元数据
9. TRAILER 跟随版本的定长数据结构
10. INDEX_V1 逻辑上的v1 hfile,应该是做兼容用的
***** 压缩后的数据大小(int)
1. 不包括header
2. 这是hfile中block的实际size,所以可以用这个来跳过当前块
3. 在v1中,只在header存了未压缩的size,因为压缩后的size可以用块之间的偏移量之差来计算得出
***** 未压缩的数据大小(int)
如果压缩算法指定为NONE,这个值和压缩大小相同
***** 上一个同类型的块的偏移量(相当于双向链表)
用来从后向前遍历,注意是同类型的上一个块
***** 可压缩的数据
**** 数据分区
***** Scan block section
线性扫描的时候用到的块,也包括索引页节点,bloom chunks
***** Non-Scan block section
1. meta 块
2. 中间索引
**** 索引块
3类索引按两种格式存储(root和非root)
***** data index 多级索引
1. root index
2. 可选的中间索引节点
3. 可选的叶节点
***** meta index 
root only
***** bloom index
root only
**** bloom filter
为了快速启动rs,相较于v1 bloom 过滤器被拆分成小的bloom chunks
***** 一个复合型bloom 过滤器
1. 版本
2. 总字节大小
3. hash 方法的number
4. hash 方法的种类
5. 总的key的数量
6. key的数量上限
7. bloom 块的数量
8. 比较器
9. bloom 索引块
**** File info
比较器属性从file info 移至trailer
| hfile.LASTKEY       | hfile的最后一个key |
| hfile.AVG_KEY_LEN   | 平均key长          |
| hfile.AVG_VALUE_LEN | 平均value长        |
**** 
** Hbase使用
*** 适用场景
- 列族结构经常调整
- 结构化数据或半结构化数据
- 需要高并发写入

*** 不适用场景 
- 需要事务 因为hbase只有行锁级别的事务
- 需要关系计算，例如 join,union,groupby，因为是列存
- 不按rowkey查询
- 高并发随机读

*** hbase shell
- 常用工具
  - 状态查询
  - ddl dml
  - 集群工具
  - replication
  - 快照
  - namespace
*** 常用列族属性
- IN_MEMORY : 在缓存中的优先级，常驻内存
*** thrift
- 跨语言的开发
*** 协处理器
**** 两类协处理器
1. observe
   - 类似触发器或回调函数
   - 在某一事件后触发
2. endpoint
   - 类似存储过程
   - 通过rpc调用rs端的计算
**** 协处理器的配置
- 全局 : jar包上传到服务端，并配置
- 用户表协处理器 : hbase shell 或api部署
- 一个参数 :避免因协处理器的异常导致rs的宕机，禁用有问题的协处理器
**** 经验
- 用处
  - 辅助监控 observer
  - 协助处理数据-索引表
  - 统计操作
    - 各region分别统计
    - 客户端归并
- 误区
  - endpoint的数据太大
  - 没有对资源占用进行估计
    - 使用过多线程 cpu
    - 使用过多内存 rs oom
** Hbase基本架构
- 客户端
- master
- regionserver
- zk
- hdfs 
** 运维
*** 集群状态信息
- web页面
- log查看
  - 是否有Exception
  - 是否有线程意外退出，exit关键字
- gclog
  - gc时间长或者full gc比较多，那么内存不够用或者配置有问题
- hbase监控痛点
  1. 组件下线
  2. unassigned region时间过长
     - zkdump或zk中获取:unassignedregion文件中有内容且超过一定时间
       - 要么有严重的容灾
       - 要么容灾卡住了
  3. regionserver处理客户端请求的时间过长
     - 某系关键线程可能卡住了
  4. storefile总数过多
     1. too many open file 异常
     2. 影响其他线程
     3. 也说明compaction线程不太行
- 工具hbck查看
  - 检测和修复数据不一致，表信息，rs挂载，hdfs目录这三类信息
  - -repair 启动所有修补项
  - 修复前确认没有正在跑的task，避免家中现有问题
  - hbck是异步的，修复后根据受影响的region数量等待一段时间，再次检测
*** 数据迁移和备份
**** 迁库需求
1. 服务器迁机房
2. 服务扩容至更大的集群
3. 集群版本升级
4. 从传统数据库导入
5. 备份
**** bulkload
**** distcp
1. hdfs 集群间的拷贝
2. 拷贝完，(0.94后)用一次hbck修复数据不一致来加载表(旧版本用addTable.rb)
**** export+import
**** copytable
- 逐条put
**** 实时备份 replication
** inbox
- 分区容忍性 : 系统中任意信息的丢失或者失败不影响整个系统的运作
  - 例如 : 一个rs挂掉了或者一个dn挂掉了，对整个系统的影响是很小的
- zk协调rs的容灾问题
- hbase releasenodes
- hbase_cleanup.sh zk有一致性问题的时候用来删除zk上的数据，有什么弊端么
- hbase 与hdfs append
- 前缀树
- 压缩算法的比较，与数据特性的关系
- bloomfilter什么时候用rowcol
- hbase 计数器
- 底层数据迁移时 版本不正确 使用bin/hbase migrate'
- 使用hfile命令行工具可以查看hfile中key所占的空间比
** protobuf
*** 一些protobuf相关的信息
+ 从protobuf .proto文件生成java文件是构建的一部分
+ proto文件在2.0以后被集中起来
+ hbase2.0以后 proto的使用变得有点复杂，为了升级到protobuf3来配合netty的零拷贝。hbase core的protobuf从HDFS独立出来。
+ 相比hbase-protocol，hbase core已经开始依赖hbase-protocol-shaded了，但是为了兼顾endpoint，还保留着hbase-protocol。以后会理清吧
*** hbase项目生成protos的java文件
1. build protocol-shaded子模块,有需要还会buildprotocol模块
#+BEGIN_SRC sh
  # pl 指定打包的模块，可以用路径也可以用坐标，如果父项目同时是父目录，那么进入父目录下，直接“-pl 项目目录名”即可
  # -am：意味着also-make （dependency），即同时打包依赖的模块；
  mvn clean install -pl hbase-protocol-shaded -am
#+END_SRC
2. 刷新maven index，找到生成的代码

** Mini-clushter

** WAL
通常，一个region上的所有table公用一个wal。但是hbase：meta有自己的专用wal
*** purpose 
用来恢复在rs宕机时还没来的及flush的memstore中的变化的数据
*** 在hdfs上的位置
+ /hbase/WALs/{每个region}
+ 暂时不要使用hadoop ec目录

*** 配置
+ hbase.wal.provider ：具体的wal实现
+ hbase.wal.meta_provider ： meta表专有的实现
*** wal Provider
**** 具体的
1. asyncfs ：基于非阻塞的dfsClient实现的wal写入器，目前重度依赖hdfs的更新，会在失败时替换会filesystem
2. filesystem ：基于老式的阻塞dfsClient
3. multiwal ： 一个rs上多个wal
*** wal splitting
当一个region open时，需要把wal中属于该region的edit 重放，因此edit需要按region分组，这个分组过程就是log splitting
**** 触发时机，master是执行者
1. start-up
2. serverShutdownHandler
**** procedure
1. The /hbase/WALs/<host>,<port>,<startcode> directory is renamed.
2. 按region分组，一个分组一个分组的读，写入到/hbase/<table_name>/<region_id>/recovered.edits/.temp，写完后.temp 重命名为该文件中的第一条log的sequence id，sequenc id用来决定replay时从哪开始
3. 分组完成后，assign region ，region open 后replay，然后flush，然后删除recovered.edits
**** 2.0以后不需要使用PV2代替zk做协调
*** wal Compression

** Region

** master
*** 职责
1. monitor所有rs
2. 保存所有metadata
*** HMasterInterface
面向metadata的方法
+ Table (createTable, modifyTable, removeTable, enable, disable)
+ ColumnFamily (addColumn, modifyColumn, removeColumn)
+ Region (move, assign, unassign)
*** 线程
**** 1. LoadBalancer

**** 2. CatalogJanitor
周期性的检查清理hbase:meta

*** MasterProcWAL
master上procedure的wal

** regionServer

*** HRegionRegionInterface
面向data和region管理
+ Data (get, put, delete, next, etc.)
+ region（master下发命令的执行者）

*** 线程
1. CompactSplitThread ： for split and minor compaction
2. MajorCompactionChecker
3. MemStoreFlusher
4. LogRoller

*** block cache
hbase提供两种不同的cache来缓存从hdfs读上来的数据块
1. on-heap LruBlockCache
2. BucketCache 通常在堆外

**** CombinedBlockCache
一个管理类，开启bucketCache，形成一个双层缓存，L2（bucketcache）在堆外缓存datablock，L1(LruBlockCache)缓存metaBlock（index block和bloom block）
**** LruBlockCache
***** 设计：内部为block划分3个优先级
1. Single access priority 初次从hdfs load上来的
2. Multi access priority 在1中再次命中的
3. In-memory 配置的常驻内存，例如meta表
***** 使用
+ 集群中总cache大小的计算：rs数 * heap size * hfile.block.cache.size * 0.99
+ hfile.block.cache.size 默认0.4
+ 0.99 lru装载因子，超过这个比例，就淘汰block
+ 频繁的eviction将会导致更多的GC
+ map一个table的时候可以选择不带blockcache的scan
**** 堆外缓存
1. 2.0以前，使用堆外缓存意味着在读取的时候先要把block从堆外copy回来，尽管避开了GC，但是速度慢了
2. 2.0以后，改变了读路径，可以直接读非堆上的block，零拷贝的读。同时又兼具自己管理GC的好处，从HBase 2.0.0起，L1和L2的概念已被弃用。当BucketCache打开时，DATA块将始终转到BucketCache，而INDEX / BLOOM块将转到堆LRUBlockCache。 cacheDataInL1支持已被删除
**** bucketBlockCache
三种模式
+ off-heap
+ file
+ mmaped file mode

** off-heap


** quota
*** 配置
+ hbase.quota.enabled 是否启用quota
+ hbase.quota.refresh.period 配置刷新周期
+ 可以提前配额或者在runtime配额

*** 基本使用
#+BEGIN_SRC sh
  # Limit user u1 to 10 requests per second
  hbase> set_quota TYPE => THROTTLE, USER => 'u1', LIMIT => '10req/sec'

  # Limit user u1 to 10 read requests per second
  hbase> set_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, USER => 'u1', LIMIT => '10req/sec'

  # Limit user u1 to 10 M per day everywhere
  hbase> set_quota TYPE => THROTTLE, USER => 'u1', LIMIT => '10M/day'

  # Limit user u1 to 10 M write size per sec
  hbase> set_quota TYPE => THROTTLE, THROTTLE_TYPE => WRITE, USER => 'u1', LIMIT => '10M/sec'

  # Limit user u1 to 5k per minute on table t2
  hbase> set_quota TYPE => THROTTLE, USER => 'u1', TABLE => 't2', LIMIT => '5K/min'


  # Limit user u1 to 10 read requests per sec on table t2
  hbase> set_quota TYPE => THROTTLE, THROTTLE_TYPE => READ, USER => 'u1', TABLE => 't2', LIMIT => '10req/sec'

  # Remove an existing limit from user u1 on namespace ns2
  hbase> set_quota TYPE => THROTTLE, USER => 'u1', NAMESPACE => 'ns2', LIMIT => NONE

  # Limit all users to 10 requests per hour on namespace ns1
  hbase> set_quota TYPE => THROTTLE, NAMESPACE => 'ns1', LIMIT => '10req/hour'

  # Limit all users to 10 T per hour on table t1
  hbase> set_quota TYPE => THROTTLE, TABLE => 't1', LIMIT => '10T/hour'

  # Remove all existing limits from user u1
  hbase> set_quota TYPE => THROTTLE, USER => 'u1', LIMIT => NONE

  # List all quotas for user u1 in namespace ns2
  hbase> list_quotas USER => ‘u1, NAMESPACE => 'ns2'

  # List all quotas for namespace ns2
  hbase> list_quotas NAMESPACE => 'ns2'

  # List all quotas for table t1
  hbase> list_quotas TABLE => 't1'

  # list all quotas
  hbase> list_quotas
#+END_SRC
*** 全局配置同时配置某个用户不生效
#+BEGIN_SRC sh
  # a per-namespace request limit
  hbase> set_quota NAMESPACE => 'ns1', LIMIT => '100req/min' 

  # user u1 is not affected by the limit
  hbase> set_quota USER => 'u1', GLOBAL_BYPASS => true


#+END_SRC
*** 配置Namespace Quotas : 
  + hbase.namespace.quota.maxtables  配置一个namespace里最多可以有多少个table
  #+BEGIN_SRC sh
    # Create a namespace with a max of 5 tables
    hbase> create_namespace 'ns1', {'hbase.namespace.quota.maxtables'=>'5'}

    # Alter an existing namespace to have a max of 8 tables
    hbase> alter_namespace 'ns2', {METHOD => 'set', 'hbase.namespace.quota.maxtables'=>'8'}

    # Show quota information for a namespace
    hbase> describe_namespace 'ns2'

    # Alter an existing namespace to remove a quota
    hbase> alter_namespace 'ns2', {METHOD => 'unset', NAME=>'hbase.namespace.quota.maxtables'}
  #+END_SRC
  + 也可以配置一个ns上有多少个region
    #+BEGIN_SRC sh
      # Create a namespace with a max of 10 regions
      hbase> create_namespace 'ns1', {'hbase.namespace.quota.maxregions'=>'10'}

      # Show quota information for a namespace
      hbase> describe_namespace 'ns1'

      # Alter an existing namespace to have a max of 20 regions
      hbase> alter_namespace 'ns2', {METHOD => 'set', 'hbase.namespace.quota.maxregions'=>'20'}

      # Alter an existing namespace to remove a quota
      hbase> alter_namespace 'ns2', {METHOD => 'unset', NAME=> 'hbase.namespace.quota.maxregions'}
    #+END_SRC
*** Space Quotas
+ 限制hbase的ns 和 table可以使用多少文件系统的存储
*** 待续。。。
* Hbase源码
** Hbase-RPC 
- 远程过程调用
** memstore
*** 一个图
#+BEGIN_SRC plantuml :file /Users/wangchao/iosdev/cheepsheets/resource/img/hbase-memstore-seq.png :cmdline -charset utf-8
  @startuml
  participant HStore as store
  participant memstore as mem

  store ->  mem ++: 使用反射，读取配置中的memstorelist生成memstore对象 
  @enduml
#+END_SRC

#+RESULTS:
[[file:/Users/wangchao/iosdev/cheepsheets/resource/img/hbase-memstore-seq.png]]
** HRegionServer
*** 重要成员
1. Map<String, HRegion> onlineRegions // 当前rs上的所有region
2. Map<String, InetSocketAddress[]> regionFavoredNodesMap // 向hdfs写hfile的时候偏向写的datanode
** Region
1. region级别的锁只有一个作用，在region正服务于其他操作时，防止其close或split
2. 每个行级操作在操作期间持有行锁和region读锁
3. 当一个scanner在构建时，getScanner持有读锁
4. 当scanner创建成功后，其持有读锁到scan结束
*** 重要成员
1. ConcurrentHashMap<HashedBytes, RowLockContext> lockedRows
2. Map<byte[], HStore> stores //
3. ReentrantReadWriteLock lock // 用于保证close
*** 重要方法
**** doMiniBatchMutate(BatchOperation<?> batchOp)
1. 获取尽可能多的行锁
** Cell
HBase中的存储单位
*** 构成
   1) row
   2) column family
   3) column qualifier
   4) timestamp
   5) type （例如 put delete等）
   6) MVCC version
   7) value
*** 

** Flush


** inbox
*** 读取流程
1. 从zk拿到meta表位置
2. 加载meta表,按rowkey查找region位置
3. 向rs发读请求
4. RegionScanner
   1. StoreScanner(列族)最小堆
      1. StoreFileScanner + MemstoreScanner最小堆
*** HFile
存储着byte array形式的kv对
**** footprint
1. 用来读写一个压缩的block的开销(常数级)
   1. 每一个compressed block需要一个编码/解码器
   2. key 的缓存buffer
   3. value的缓存buffer
2. 正比于data block的hfile index
**** 调优建议
***** 适当的block size
推荐大小文8k-1M
1. 如果使用场景主要是扫描遍历操作,则使用大一点的block size
2. 如果点查较多,则使用较小的block size.因为一个点查命中一个block后,block size越小,则解压的代价越小
3. 但同时也要考虑,同样的数据量下,越小block size,会带来越多的block,也即越多的block index.

* region 切分

* mob存储
moderate object storage
为了减少大值对象在参与频繁的compaction和split带来的性能损耗，hbase为mob对象设计了独立的I/O path

* test
#+begin_src plantuml :file tryout.png
  Alice -> Bob: synchronous call
  Alice -> Bob: asynchronous call
#+end_src

#+RESULTS:
[[file:tryout.png]]

* hbase failover 与MTTR2
某一个rs发生宕机后，hbase进入failover过程。目的是将region转移到其他rs上，并恢复宕机rs的memtable中的数据，然后提供region服务

** 步骤
1. split wal-log. 一个rs上的所有region将wal写入同一个wal文件，因此在region转移的时候，要将该region的wal从rs的wal中拆出来

2. Assign region. 重新指派rs来load region

3. replay log

** MTTR2

* hbase主备集群
此种模式下的两个核心问题
1. 数据复制
2. 流量切换
   
** 数据复制

* 受益良多
[[https://dbaplus.cn/news-73-2859-1.html][ali-hbase-2019总结]]

