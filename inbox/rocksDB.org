#+title: rocksDB
* rocksDB的历史
** 大数据分析所涉及到的点
  + 大范围顺序读
  + 两个或多个数据集的join
  + 很少量的更新
** 机械磁盘（spinning disk）的年代
通过不断优化，hbase的查询仅仅是MySql的两倍，IOPs是3倍
** 闪存(Flash storage)的出现
2012年的hbase和hdsf因为一些软件瓶颈，[[http://hadoopblog.blogspot.com/2012/05/hadoop-and-solid-state-drives.html][detail]]，无法很好的利用闪存。因而一款面向告诉闪存的KV存储引擎呼之欲出
** 为什么我们需要一个嵌入式（Embedded）数据库？
+ 设：我们对机械磁盘的一次读写耗时10 milliseconds，对闪存的一次读写耗时100 microseconds。两者差100倍，两台机器间的网络延迟为50 microseconds
+ 则 : 机械磁盘的时代，一次cs架构的数据存取只比本地存取多负担0.05ms/10ms=0.5%
+ 而 : 闪存出现后，一次cs架构的数据存取比本地存取多负担50microseconds/100microseconds=50%
+ 所以：我们需要一个嵌入式数据库[[http://en.wikipedia.org/wiki/Embedded_database][detail]]，因为它在闪存cs架构下依然会有很低的延迟
** 向闪存更新数据block总是会存在写放大(write-endurance)
** levelDB有什么不足?
+ 必须fit in OS page cache
+ 写放大严重
+ 跟不上日趋快速的存储硬件
** 展望rocksDB
1. 嵌入式KV存储，支持点查和范围查
2. 更适应于告诉存储硬件
3. 完整的数据库服务支持
4. 随着cpu和IOPs的线性成长
5. rocksDB不是一个分布式数据库，由上层应用实现replication、fault-tolerance、sharding
** RocksDB架构
todo
* using rocksDB 
1. use shell
** 在mac上安装使用rocksdb
安装
#+BEGIN_SRC sh
  brew install rocksdb
# 安装完的可执行文件
-r-xr-xr-x   1 wangchao  staff  5031508 Mar  5 23:20 rocksdb_dump
-r-xr-xr-x   1 wangchao  staff  5029696 Mar  5 23:20 rocksdb_ldb
-r-xr-xr-x   1 wangchao  staff  5075428 Mar  5 23:20 rocksdb_repl_stress
-r-xr-xr-x   1 wangchao  staff  5082536 Mar  5 23:20 rocksdb_sanity_test
-r-xr-xr-x   1 wangchao  staff  5029696 Mar  5 23:20 rocksdb_sst_dump
-r-xr-xr-x   1 wangchao  staff  5280732 Mar  5 23:20 rocksdb_stress
-r-xr-xr-x   1 wangchao  staff  5031508 Mar  5 23:20 rocksdb_undump
-r-xr-xr-x   1 wangchao  staff  5080924 Mar  5 23:20 rocksdb_write_stress
  pwd
/usr/local/Cellar/rocksdb/6.6.4/bin
#+END_SRC
** 在mac上编译rocksdb
#+BEGIN_SRC sh
  git clone
  make all
  ./db_bench fillseq --compression_type=none

./db_bench --benchmarks="readrandom" --use_existing_db --compression_type=none --statistics
#+END_SRC
* rocksdb 备份

* rocksdb 读写缓存管理 Write Buffer Manager
WriteBufferManager可以用来控制多列族和多个DB实例所共享的写缓存。在使用方式上分为两种
1. 单独限制总的写缓存 WriteBufferManager(size)
2. 在blockCache中计算Memtable消耗的写缓存，这样读写缓存可以限制在一个阈值之下,WriteBufferManager(size, cache)

一个WriteBufferManager实例可以传给column families 或者整个DB
** 第一种：限制总的写缓存
创建WriteBufferManager时需要传入一个内存限制，rocksdb会尝试将总的写缓存限制在这个阈值之下

在5.6之后，flush一个cf的触发条件有如下两种
1. 如果所有的mutable memtable(正在插入数据的memtable)的size超过阈值的90%
2. 如果总的内存超过限制，而mutable memtable size超过限制的50%，才会采取更为激进的flush

5.6之前总内存消耗在arena区中的计算是实际使用的
5.6之后总内存消耗在arena区中的计算是分配的，即使尚未使用
** 第二种：在block cache 中计算写缓存（memtable占用的内存）
如果要完整控制整个rocksdb的内存占用，建议WriteBufferManager(size, cache) 这种方式。
同时打开cache_index_and_filter_blocks = true。
这样，整个memtable, datablock, index&filter block都能被blockcache的容量限制

从5.6开始，用户可以设置rocksdb在blockcache中计算memtables的内存消耗，如此不管写缓存总限制设置与否，都能够对rokcsdb整体内存做限制
总的来说，block cache用于读的实际占用偏小，所以可以写缓存和index、filter都放进来
* 调优
rocksdb可调参数非常繁多，且彼此之间相互影响
** 案例
1. 随着compaction的发生，rocksdb的写入变慢，反映到引用为写入的时候存在抖动。
2. 查看日志没有发现写失速，compaction stats同样没有写失速的记录
** 手段
*** 牺牲一部分读
Block Size 默认值为 4KB，文档中建议生产环境调整到 16 ~ 32 KB；如果采用机械硬盘（HDD）来存放 SST 文件，那么在内存容量够用的情况下，可以调整为 128 KB，这样单次读取的数据量可以多一些；
而且 Block 变大后，相同的总数据量下，索引所占用的内存会减少。可以一起调大blockCache的大小，以保证缓存的block数量不会减少太多 
** 参数
[[https://xiking.win/2018/12/05/rocksdb-tuning/][参考]]
rocksdb是lsm架构，后台有两种工作线程：flush & compaction。flush的优先级比compaction的优先级高，两种线程池都可配。
*** max_background_flushes
是后台memtable dump成sstable的并发线程数。默认是1，但是当使用多个column family时，内部会存在多个memtable，
可能会同时发生flush，如果线程是1，在写入量大的情况下，可能会导致flush不及时，出现无法写入的情况。
*** filter_policy
这个就是每个sstable的bloom filter，使用bloom filter可以大幅减少不必要的磁盘IO。
在bits_per_key为10的情况下，bloom filter错误率估计为1%，也就是存在如下情况：有1%的概率出现错误，
key实际上不存在，但是在bloom filter查询的结果是存在的。这种情况导致会有1%的不必要的磁盘IO。
*** block_cache
可以配置大小的LRU cache，用来缓存未经过压缩的block。由于访问cache需要加锁访问，当大部分数据都在cache中时，
多线程并发访问cache可能会出现锁竞争的瓶颈，所以LRU cache还有一个shard_bits参数，将LRU cache分片，
其实就是对锁进行拆分，不同分片的cache不会出现竞争。默认shard_bits是6，那么cache的shard数目就是2^6=64。
** io限速
[[https://www.modb.pro/db/58514][rokcsdb io 限速参考]]

