#+title: rocksDB
* rocksDB的历史
** 大数据分析所涉及到的点
  + 大范围顺序读
  + 两个或多个数据集的join
  + 很少量的更新
** 机械磁盘（spinning disk）的年代
通过不断优化，hbase的查询仅仅是MySql的两倍，IOPs是3倍
** 闪存(Flash storage)的出现
2012年的hbase和hdsf因为一些软件瓶颈，[[http://hadoopblog.blogspot.com/2012/05/hadoop-and-solid-state-drives.html][detail]]，无法很好的利用闪存。因而一款面向告诉闪存的KV存储引擎呼之欲出
** 为什么我们需要一个嵌入式（Embedded）数据库？
+ 设：我们对机械磁盘的一次读写耗时10 milliseconds，对闪存的一次读写耗时100 microseconds。两者差100倍，两台机器间的网络延迟为50 microseconds
+ 则 : 机械磁盘的时代，一次cs架构的数据存取只比本地存取多负担0.05ms/10ms=0.5%
+ 而 : 闪存出现后，一次cs架构的数据存取比本地存取多负担50microseconds/100microseconds=50%
+ 所以：我们需要一个嵌入式数据库[[http://en.wikipedia.org/wiki/Embedded_database][detail]]，因为它在闪存cs架构下依然会有很低的延迟
** 向闪存更新数据block总是会存在写放大(write-endurance)
** levelDB有什么不足?
+ 必须fit in OS page cache
+ 写放大严重
+ 跟不上日趋快速的存储硬件
** 展望rocksDB
1. 嵌入式KV存储，支持点查和范围查
2. 更适应于告诉存储硬件
3. 完整的数据库服务支持
4. 随着cpu和IOPs的线性成长
5. rocksDB不是一个分布式数据库，由上层应用实现replication、fault-tolerance、sharding
** RocksDB架构
todo
* using rocksDB 
1. use shell
** 在mac上安装使用rocksdb
安装
#+BEGIN_SRC sh
  brew install rocksdb
# 安装完的可执行文件
-r-xr-xr-x   1 wangchao  staff  5031508 Mar  5 23:20 rocksdb_dump
-r-xr-xr-x   1 wangchao  staff  5029696 Mar  5 23:20 rocksdb_ldb
-r-xr-xr-x   1 wangchao  staff  5075428 Mar  5 23:20 rocksdb_repl_stress
-r-xr-xr-x   1 wangchao  staff  5082536 Mar  5 23:20 rocksdb_sanity_test
-r-xr-xr-x   1 wangchao  staff  5029696 Mar  5 23:20 rocksdb_sst_dump
-r-xr-xr-x   1 wangchao  staff  5280732 Mar  5 23:20 rocksdb_stress
-r-xr-xr-x   1 wangchao  staff  5031508 Mar  5 23:20 rocksdb_undump
-r-xr-xr-x   1 wangchao  staff  5080924 Mar  5 23:20 rocksdb_write_stress
  pwd
/usr/local/Cellar/rocksdb/6.6.4/bin
#+END_SRC
** 在mac上编译rocksdb
  git clone
#+BEGIN_SRC sh
  make all
  ./db_bench fillseq --compression_type=none

./db_bench --benchmarks="readrandom" --use_existing_db --compression_type=none --statistics
#+END_SRC
* rocksdb 备份

* rocksdb 读写缓存管理 Write Buffer Manager
WriteBufferManager可以用来控制多列族和多个DB实例所共享的写缓存。在使用方式上分为两种
1. 单独限制总的写缓存 WriteBufferManager(size)
2. 在blockCache中计算Memtable消耗的写缓存，这样读写缓存可以限制在一个阈值之下,WriteBufferManager(size, cache)

一个WriteBufferManager实例可以传给column families 或者整个DB
** 第一种：限制总的写缓存
创建WriteBufferManager时需要传入一个内存限制，rocksdb会尝试将总的写缓存限制在这个阈值之下

在5.6之后，flush一个cf的触发条件有如下两种
1. 如果所有的mutable memtable(正在插入数据的memtable)的size超过阈值的90%
2. 如果总的内存超过限制，而mutable memtable size超过限制的50%，才会采取更为激进的flush

5.6之前总内存消耗在arena区中的计算是实际使用的
5.6之后总内存消耗在arena区中的计算是分配的，即使尚未使用
** 第二种：在block cache 中计算写缓存（memtable占用的内存）
如果要完整控制整个rocksdb的内存占用，建议WriteBufferManager(size, cache) 这种方式。
同时打开cache_index_and_filter_blocks = true。
这样，整个memtable, datablock, index&filter block都能被blockcache的容量限制

从5.6开始，用户可以设置rocksdb在blockcache中计算memtables的内存消耗，如此不管写缓存总限制设置与否，都能够对rokcsdb整体内存做限制
总的来说，block cache用于读的实际占用偏小，所以可以写缓存和index、filter都放进来
* rocksdb Compaction
** compaction_reason
6.25.1版本有如下compaction reason
1. "Unknown"
2. "LevelL0FilesNum"
3. "LevelMaxLevelSize";
4. "UniversalSizeAmplification";
5. "UniversalSizeRatio";
6. "UniversalSortedRunNum";
7. "FIFOMaxSize";
8. "FIFOReduceNumFiles";
9. "FIFOTtl";
10. "ManualCompaction";
11. "FilesMarkedForCompaction";
12. "BottommostFiles";
13. "Ttl";
14. "Flush";
15. "ExternalSstIngestion";
16. "PeriodicCompaction";
17. "ChangeTemperature";
*** PeriodicCompaction
** CompactionJob
负责执行compaction,通常的步骤`Prepare()`->`Run()`->`Install(),可以使用subcompaction来并行编排任务
** CompactionPicker
如果NeedCompaction返回true,则调用PickCompaction()来选择要compaction的文件和输出的位置

CompactRange()和CompactFile可以作为手动触发compaction的实现
** compaction 触发时机
1. memtable flush到l0后
2. 手动触发compaction
** compaction 分类
*** 自动compaction
*** 手动compaction
当db接到一个manual-compaction请求后,就暂时不会编排新的自动compaction任务,现在这个行为可以通过CompactRangeOptions::exclusive_manual_compaction来
** periodicCompactionSeconds和ComputeExpiredTtlFiles的区别
*** ComputeExpiredTtlFiles
1. 在columnFamilyOptions上设置ttl,则rocksdb在计算Compactionscore的时候会检查除最后一层外的所有文件
2. 计算一个sst文件最开始被flush时的时间作为其访问时间,然后用这个时间取和ttl比
*** ComputeFilesMarkedForPeriodicCompaction
按下面的优先级来决定创建时间
1. 使用sst文件的创建时间,首先使用文件创建时间.这样可以避免一个sst在参与每次的compaction,因为其参与一次compaction后构成一个新的sst,拥有更新的创建时间q
2. 使用sst文件被flush的时间,因为这个sst可能合并的结果,需要找一个源sst的时间,这里需要考虑6.6.0前flush的时候是否在FileMetaData中记录了这个时间
3. 使用sst的修改时间
4. 与上面的方法不同的是,这个方法会检查每一层,不会放掉最后一层

* 调优
rocksdb可调参数非常繁多，且彼此之间相互影响
** 案例
1. 随着compaction的发生，rocksdb的写入变慢，反映到引用为写入的时候存在抖动。
2. 查看日志没有发现写失速，compaction stats同样没有写失速的记录
** 手段
*** 牺牲一部分读
Block Size 默认值为 4KB，文档中建议生产环境调整到 16 ~ 32 KB；如果采用机械硬盘（HDD）来存放 SST 文件，那么在内存容量够用的情况下，可以调整为 128 KB，这样单次读取的数据量可以多一些；
而且 Block 变大后，相同的总数据量下，索引所占用的内存会减少。可以一起调大blockCache的大小，以保证缓存的block数量不会减少太多 
** 参数
[[https://xiking.win/2018/12/05/rocksdb-tuning/][参考]]
rocksdb是lsm架构，后台有两种工作线程：flush & compaction。flush的优先级比compaction的优先级高，两种线程池都可配。
*** max_background_flushes
是后台memtable dump成sstable的并发线程数。默认是1，但是当使用多个column family时，内部会存在多个memtable，
可能会同时发生flush，如果线程是1，在写入量大的情况下，可能会导致flush不及时，出现无法写入的情况。
*** filter_policy
这个就是每个sstable的bloom filter，使用bloom filter可以大幅减少不必要的磁盘IO。
在bits_per_key为10的情况下，bloom filter错误率估计为1%，也就是存在如下情况：有1%的概率出现错误，
key实际上不存在，但是在bloom filter查询的结果是存在的。这种情况导致会有1%的不必要的磁盘IO。
*** block_cache
可以配置大小的LRU cache，用来缓存未经过压缩的block。由于访问cache需要加锁访问，当大部分数据都在cache中时，
多线程并发访问cache可能会出现锁竞争的瓶颈，所以LRU cache还有一个shard_bits参数，将LRU cache分片，
其实就是对锁进行拆分，不同分片的cache不会出现竞争。默认shard_bits是6，那么cache的shard数目就是2^6=64。
** io限速
[[https://www.modb.pro/db/58514][rokcsdb io 限速参考]]
* 案例
** rocksdb ttl
*** ttl db
rocksDB提供ttl db的实现,大概就是用这个ttl db实例去如写入数据的时候会在写入的数据上标记入库时间,同时增加TtlCompactionFilter.
这样当我们对db或者对cf设置了ttl后,sst文件在参与compaction的时候会被该TtlCompactionFilter检查并过滤掉过期条目,以达到删除过期数据
的目的
**** 缺陷
1. 非精准过期,数据条目真正过期依赖于其参与compaction的时机
2. 在数据被真正删除之前,即便是老于设置的ttl了还是可以查到
**** 问题
- 线上rocksdb版本 5.1.18
- sr初始ttl为P100D(实际上我们查rokcsdb中的数据发现1年以前的数据依旧可查),后因rokcsdb磁盘占用过高,调整为P30D,希望rocksdb能够释放一些磁盘.
***** 初始rocksdb中各层数据分布如下(测试环境模拟的,与这个有所不同,线上只有4层,数据90%堆在最底层)
   #+begin_src text
    1. ttl = P100D

    ,** Compaction Stats [T_Server_TraceData] **
    Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop
    ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      L0      1/0   40.84 MB   0.0      0.4     0.0      0.4       1.2      0.8       0.0   2.8      4.9     14.5     83.88             80.54        73    1.149   8076K     34
      L1      1/0   61.14 MB   0.0      0.2     0.2      0.1       0.4      0.3       0.0   1.9     18.2     26.6     13.94             12.56         5    2.789   3903K     19
      L2      1/0   61.43 MB   0.0      0.2     0.2      0.1       0.3      0.3       0.0   2.2     18.6     27.7     12.86             11.34         4    3.215   3810K     21
      L3      1/0   97.95 MB   0.0      0.4     0.3      0.1       0.5      0.4       0.0   1.7     26.2     32.5     16.87             17.89         4    4.216   5734K     42
      L4      6/0    1.02 GB   0.0      1.4     0.9      0.5       1.4      0.9       0.0   1.5     37.8     37.8     36.78             51.90         2   18.389     14M     40
      L5     10/0    1.87 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0
      L6     32/0    7.66 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0
     Sum     52/0   10.80 GB   0.0      2.7     1.6      1.1       3.8      2.7       0.0   8.8     16.6     23.6    164.33            174.24        88    1.867     36M    156

  #+end_src
***** ttl调小后发现磁盘占用并没有下来
1. 观察rocksdb日志(关键字为"compacted to" + "dropped"),发现compaction发生的很少,dropped掉的条目也很少,明显想要过期的数据并没有被删掉
2. 检查rokcsdb配置发现  rocksdb.max.bytes.level.base 为2G, 在mutable_cf_options.max_bytes_for_level_multiplier默认为10的情况下,L4层的sst文件会在接近2T的时候触发compaction
   所以怀疑L4层大量含有过期数据的sst文件并没有参与到compaction中,也就没法被过滤,导致磁盘没有被释放
3. 所以调小 max.bytes.level.base为256M,希望L4层的数据有机会参与到compaction中
4. 调小后,发现前3个leve的compaction频率上来了,但是磁盘占用并没有下来多少
5. 检查日志(关键字"compacted to")后发现,Compaction Stats多了个L5,L4数据全都写入了L5(10分钟左右),但是compaction event却是trivial_move
   怀疑L4层的sst直接转移到了L5.
6. 翻看rokcsdb compaction的文档,发现rocksdb的level_compaction做合并的时候是选出当前层的几个文件和下一层中有overlap的文件做合并,
   而因为我们大量的key是按时间写入的,对数据的操作访问也集中在写数据后很短的时间,所以对于过期的数据,其之间只存在很少的overlap,即大量的老sst还是没有机会参与到合并中
***** 手动调用db.compactRange
该函数会对每个文件都做一次compaction,但是缺点是会停掉本来的compaction线程,可能会造成write stall
***** 6.6.0新特性
6.6.0以后,rocksdb为了缓解沉底的sst基本参与不到compaction中的问题,引入了两个新的参数
1. options.ttl 列族上的参数,让老sst在compaction线程不忙的时候有机会参与compaction
2. options.periodic_compaction_seconds, 如果有compaction filter存在,则该参数保证一个sst在一个时间周期内参与compaction
** rocksdb 向dropped的cf里写入数据会导致该db实例报错
Invalid column family specified in write batch
rocksdb在使用时需要存储column family handler,如果没有做同步限制,则在多线程环境下,当一个线程drop掉了一个cf,另一个线程使用
存储的column family handler来put的时候,会导致整个db实例报错

