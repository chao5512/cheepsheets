#+title: druid
* OLTP versa OLAP
** Online transaction processing
主要用于事务性的处理，需要支持频繁的增删该查
** Online analytical processing
主要提供查询、支持复杂分析、侧重决策支持
* druid的数据结构
** datasource结构
druid在入库的时候对数据进行聚合，聚合从两个角度进行，某段时间内的某些维度。不开启rollup可关闭预聚合保留明细。druid不适合做明细查询
1. 时间列
2. 维度列
3. 指标列
** segment结构
1. slice： segment按时间范围存储druid数据，这完成了横向切分数据
2. dice： 在每个segment里，druid按列压缩存储数据，这完成了纵向切分数据
*** segment相关定义
**** DataSegment
代表segment元数据
**** Segment
代表数据，可查


* 设计
** 设计期望
1. 多进程
2. 易扩展
3. 容灾能力，某一个进程的崩溃不会立刻影响其他节点
** 设计思想
既保证查询的高效性（写入时建立索引），又保证写入的高效性
** 架构
*** 节点角色
1. coordinator： 管理segement
2. overload： 数据摄取的任务分配，提交任务、取消任务、查看任务状态、汇报状态
3. broker： 处理外部查询请求，按segment转发请求到历史节点或实时节点，lru
4. router： 可选节点，可以将请求路由到其他节点，运行web控制台
5. historical： 提供不可变segment的加载和查询功能
6. middleManager: 负责摄取数据
*** 外部依赖
**** kafka
消息总线，供实时节点摄取数据用
***** 提供如下功能
1. 内存中的数据安全，real-time宕机后丢失的内存数据仍在kafka中可以找到
2. 内存加磁盘中的数据安全，多个real-time可以同时摄取kafka中的事件，构成事件副本，当某个real-time完全不可用（内存丢了，磁盘坏了）

**** deep storage
1. 用作数据备份
2. 用作数据传递的共享存储
3. historical在响应查询的时候应该是不访问深度存储的，他应该在查询前就欲拉取segments到本地磁盘，待验证：不过懒加载模式下，应该是query时才去加载
**** 元数据存储
1. 存储segment使用信息和任务信息
2. 规则
**** zookeeper
1. 服务发现
2. 协调，segment的加载和卸载指令
3. leader选举

*** 存储
**** datasource
1. 数据存储在datasource中，可以看作一张表，表中数据按时间分区
2. 每个分区被称为chunk
3. 每个chunk中，数据又被分区成为一个或多个segement
**** segment
***** middle manager
segment是在middlemanager上创建的，在middlemanager上时，它是可变的、未提交的
***** segment创建步骤
1. 转换为列式
2. Indexing：创建位图索引，并使用索引替换维度列
3. 压缩
   1. 对字符串的压缩
   2. 对位图索引的压缩
   3. 对所有拥有类型声明的列的压缩
***** segment周期的提交与发布
1. segment刷入deep storage
2. handoff: segment被托管给historical
3. 写入segment相关信息到元数据，用来决定加载或写在segment

* 算法
** bitmap
[[http://hbasefly.com/2018/06/19/timeseries-database-8/][hbaseFly]]
* utils
** lifecycle
** NativeIO
ruid依赖linux的页缓存来缓存segment，但这两种情况的segment是不应该对页缓存产生大的占用的
1. 从deep storage 拉取segment
2. 只是为了rebalence segment而分配来的segment
优化： 使用sync_file_range来代替fsync来加速
* 用到的工具
** jackson
[[https://developer.ibm.com/zh/articles/jackson-advanced-application/][ibm-ref]]
* develop
** start point
数据存在segment的列中,可以从Column.java(在#5957中重命名为ColumnHolder.java)和继承它的类入手来了解存储格式.
** segment creation
1. IncrementalIndex.java 摄取数据
2. IndexMerger.java 创建segment
** storage engine
segment使用IndexIO.java映射内存,并为查询提供数据
** query engine
query逻辑去Query* 类看. 可以从QueryResouse.java下手
** coordination
1. historical的协调逻辑从DruidCoordinator.java下手
2. 实时摄取的协调逻辑从OverloadResource.java下手
** real-time ingestion
1. druid使用FirehoseFactory.java类来加载数据
2. hand-off 逻辑 RealtimePlumber.java
** hadoop-based batch ingestion
1. 决定创建多少segment  HadoopDruidDetermineConfigurationJob.java
2. 创建segment HadoopDruidIndexerJob.java
* paper
** 实时节点
*** 数据流动的4个阶段
1. ingest
2. persist
3. merge -> segment
4. hand off
*** 窗口
最小化丢数据的风险
1. 在窗口末尾去merge所有的persist index并执行hand off
2. 一旦这个窗口内的数据在historical节点可查了,刷出窗口内的所有信息,并撤销当前节点对这个窗口的server
** message bus
1. 用作消息事件的缓存,以便在failure和recover场景下正确消费消息
2. 统一的数据来源,多个消费节点可以冗余消费提升灾备能力或者分区消费提升消费速度
** 历史节点
遵循shared-nothing architecture
1. historical 彼此不认识
2. 只知道load, drop, server immutable segment
3. 和实时节点一样,通过zk发布服务状态和服务的数据
*** 分层
对historical分组,加载不同要求的segment.可以用来做冷热分离,配合规则
** 代理(broker)节点
理解zk上的segment信息,代理收到的查询,到正确的节点上收集数据,并汇总出最终结果.响应查询
*** LRU
用来缓存从历史节点查询指定segment的结果,不缓存从实时节点来的数据
** 协调节点
** mysql
1. 存需要由historical 节点接管的segment
* dump segments
2. 存规则,关于segment怎么创建? 怎么销毁? 怎么保存副本?[
[https://xixuebin.github.io/2019-04-01-092732-ch.html][使用DruidAPI dump Druid数据]]
* druid中的zk
** zk的用处
1. 协调节点和Overlord节点选主
2. 历史节点发布segement
3. 协调节点分配管理（load、drop）历史节点上的segment
4. Overlord和MiddleManager的任务管理
** 路径
- coordinator选主,overlord同理
#+begin_src bash
  [zk: localhost:2181(CONNECTED) 8] get /druid/cluster/coordinator/_COORDINATOR/_c_da02aa4f-2450-4616-923e-def24f6e0ba9-latch-0000000081


  # http://10.240.3.161:8081

#+end_src

- 历史节点和实时节点的服务发现，此处节点为临时节点
#+begin_src bash
  [zk: localhost:2181(CONNECTED) 35] ls /druid/cluster/announcements

  # [datanode01:8083, managernode01:8100, managernode01:8101, managernode01:8102, writenode01:8101, writenode02:8083, writenode02:8100, writenode02:8101, writenode02:8102]

#+end_src

- 历史节点加载的segment,此处历史节点为永久节点，其上加载的segment为临时节点
#+begin_src bash
[zk: localhost:2181(CONNECTED) 25] ls /druid/cluster/segments/datanode01:8083
# _segment_identifier_
#+end_src

- segment分配
#+begin_src bash
  # 要求datanode01:8083去load或者drop的segment
  [zk: localhost:2181(CONNECTED) 40] ls /druid/cluster/loadQueue/datanode01:8083/_identifier_segment

#+end_src
* Storage design
** Indexing and handoff
- indexing是将传入的数据组织成segment的机制
- handoff是将segment发布并使历史节点提供数据服务的机制
*** Indexing (任务执行节点)
在开始build segment之前,需要先确定segment identifier.
**** 如何获得identifier
1. 对于append模式的任务:在OverLord上调用allocate api来添加一个新的分区到一个已有的segments集合
2. 对于overwrite模式的任务:lock一个interval,并创建一个新的版本号和一个新的segment集合
**** 可查性
实时任务摄取的数据是实时可查的,只不过还没有发布
**** index 任务完成后
push到deep storage 并发布(在metastore中写一条元数据)segment.如果这个任务是realtime-task,则其支持查询
指导有历史节点成功加载了这些segment,然后退出任务.其他任务则直接退出.
*** 在Coordinator / Historical端
1. coordinator周期的拉取metastore中的元数据,期待得到新发布的segment
2. 当coordinator发现了segment that is published and used, but unavailable.coordinator选择
   一个历史节点并命令该历史节点加载这个segment
3. 历史节点load segment并开始提供该segment的数据服务
4. 此时,如果indexing task正在等待handoff完成,那么它可以退出了
** segment identifier
identifier由4部分组成
1. Datasource name
2. Time Interval (在创建摄取任务时指定,参数为SegmentGranularity)
3. Version Number(任务开始时的时间戳用作版本号)
4. Partition Number
*** 例子
#+begin_src text
  clarity-cloud0_2018-05-21T16:00:00.000Z_2018-05-21T17:00:00.000Z_2018-05-21T15:56:09.909Z_1
#+end_src
** Segment versiong
用来支持batch-mode overwriting。当高版本的segment全部加载完成后，查询才会被导向高版本的segment
** Segment lifecycle
segment的生命周期涉及3个地方
1. Metadata store ：segment写入metadata store的动作叫publishing。used(用作删除标记)字段标示该segment是否用于查询
2. Deep stoarage ： 在segment publish之前将该segment推入Deep storage
3. 查询可用性 ： segment在历史节点或者实时节点上以供查询
** Availability and consistency
druid的摄取和查询在结构上分离，所以考察可用性和一致性的时候要分开看
*** ingestion
druid摄取是pull-based，提供事务保障。
**** 对于从kafka这类流式摄取
druid在publish segment的到元数据的同一个事务中同时存一个流offset。如果某个摄取任务失败，那么它已经摄取的部分数据将会被丢弃，
druid开始一个新的摄取任务，这个摄取任务从最后一次成功提交的segment记录的offset继续摄取
**** 基于hadoop的批量摄取
所有的segment元信息在同一个事务中发布
**** 本地批量摄取
1. parallel mode: 在子任务完成后publish所有的segment到metastore
2. simple mode： 所有的segment的元信息一起提交
*** query
broker负责在查询的时候来保证查询涉及的segments的一致性
1. atomic replacement 每个time chunk单独的完成原子替换
* Segment
** schema change

*** version变化
1. 保证interval级的原子性

*** 同一个datasource的不同schema
缺少的维度会按默认值算,例如string类型为null

** Sharding
同一个datasource的同一个interval可能会有多个segment.可以在摄取的时候指定shardSpec来shard data.
同一个interval的多个segment组成一个block.除了linear shardSpec,block表现为只有当block中的segments都load后才可查.

*** ShardSpec
1. none
2. single
3. linear
4. numbered
5. hashed
6. numbered_overwrite
7. building_numbered
8. building_hashed
9. building_single_dim
10. bucket_hash
11. bucket_single_dim
* Task
在druid中,task做的都是ingestion相关的工作
1. 对于批量任务,是直接使用 Task Api提交任务到druid
2. 对于流摄取任务,使用supervisor提交
** Task Api
在两个地方使用
1. 在 **overlord**进程上使用Http Api可以提交任务、取消任务、查看任务状态等
2. druid sql有一张只读的 **sys.tasks* 的表,里面有任务全量信息的子集

* 源码
** AppenderatorDriverRealtimeIndexTask
** FireDepartment
FireDepartment由Firehose和Plumber组成
*** 隐喻
1. Firehose:实时数据流，提供realtime stream data
2. Plumber:水管工，将来自Firehose的水流导向正确的sink，并保证sink不会溢出(需要对水流有控制能力)
   

* 工作
1. checkpoint
2. compact
3. 资源管理(内存、线程)
* 翻译
** 去掉windowPeriod
去掉windowPeriod的限制，可以接受任何时段的数据
*** 难点
1. 即便是在晚来数据特别小的时候，也要保持“segment大致大小相等”的特性，这依赖类似于compaction的进程
2. Query-Consistency。在用户视角来看，insert是平滑的，compaction是原子的 **why need?**
*** MVP Proposal: Expandable historical segment sets
可扩展的历史segement集

1. 
** FirehoseFactories
最初的设计是面向消息(一行一行的)的
** InputSource
是存储input data的存储系统的抽象,可能需要提供一个InputFormat来解释不同格式的input data
*** 数据的格式
1. csv
2. json
3. regex
4. tsv

