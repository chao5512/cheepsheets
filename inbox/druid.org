#+title: druid
* OLTP versa OLAP
** Online transaction processing
主要用于事务性的处理，需要支持频繁的增删该查
** Online analytical processing
主要提供查询、支持复杂分析、侧重决策支持
* druid的数据结构
** datasource结构
druid在入库的时候对数据进行聚合，聚合从两个角度进行，某段时间内的某些维度。不开启rollup可关闭预聚合保留明细。druid不适合做明细查询
1. 时间列
2. 维度列
3. 指标列
** segment结构
1. slice： segment按时间范围存储druid数据，这完成了横向切分数据
2. dice： 在每个segment里，druid按列压缩存储数据，这完成了纵向切分数据
   

* 设计
** 设计期望
1. 多进程
2. 易扩展
3. 容灾能力，某一个进程的崩溃不会立刻影响其他节点
** 设计思想
既保证查询的高效性（写入时建立索引），又保证写入的高效性
** 架构
*** 节点角色
1. coordinator： 管理segement
2. overload： 数据摄取的任务分配，提交任务、取消任务、查看任务状态、汇报状态
3. broker： 处理外部查询请求，按segment转发请求到历史节点或实时节点，lru
4. router： 可选节点，可以将请求路由到其他节点，运行web控制台
5. historical： 提供不可变segment的加载和查询功能
6. middleManager: 负责摄取数据
*** 外部依赖
**** kafka
消息总线，供实时节点摄取数据用
***** 提供如下功能
1. 内存中的数据安全，real-time宕机后丢失的内存数据仍在kafka中可以找到
2. 内存加磁盘中的数据安全，多个real-time可以同时摄取kafka中的事件，构成事件副本，当某个real-time完全不可用（内存丢了，磁盘坏了）

**** deep storage
1. 用作数据备份
2. 用作数据传递的共享存储
3. historical在响应查询的时候应该是不访问深度存储的，他应该在查询前就欲拉取segments到本地磁盘，待验证：不过懒加载模式下，应该是query时才去加载
**** 元数据存储
1. 存储segment使用信息和任务信息
2. 规则
**** zookeeper
1. 服务发现
2. 协调，segment的加载和卸载指令
3. leader选举

*** 存储
**** datasource
1. 数据存储在datasource中，可以看作一张表，表中数据按时间分区
2. 每个分区被称为chunk
3. 每个chunk中，数据又被分区成为一个或多个segement
**** segment
***** middle manager
segment是在middlemanager上创建的，在middlemanager上时，它是可变的、未提交的
***** segment创建步骤
1. 转换为列式
2. Indexing：创建位图索引，并使用索引替换维度列
3. 压缩
   1. 对字符串的压缩
   2. 对位图索引的压缩
   3. 对所有拥有类型声明的列的压缩
***** segment周期的提交与发布
1. segment刷入deep storage
2. handoff: segment被托管给historical
3. 写入segment相关信息到元数据，用来决定加载或写在segment
****** Indexing过程
******* 需要在构建所以前确定segment的标识符
1. append模式,使用overload的allocate api来添加一个新的分区到一个已有的segments集合
2. overwrite模式，新的版本号，新的segment集合
******* 实时任务
支持实时查询，但是并没有发布到深度存储
******* index完成时
可以发布同时向元数据存储写入一条该segment相关数据
******* indexing完成后
如果indexing是个实时任务，那么它将等待historical节点加载该segment。不是实时任务则直接退出index阶段
******* 此时coordinator和historical的一面
1. coordinator周期的拉取元数据中新发布的segment
2. 发现新发布但是并未可用的segment，挑选一个historical并命令该historical节点加载segent并提供服务
3. historical节点干活
4. 这时如果index 任务还在等待handoff， 那么它可以退出了
****** segment identifier
datasource_intervalStart_intervalEnd_version_partitionNum
1. datasource name
2. Time interval，包含这个segment的chunk的时间段。对应对摄入任务的segmentGranularity，granularitySpec
3. version number
4. 分区号。ok now we can find any segment
***** segment lifecycle
****** 三个方面
******* 元数据存储
1. 向元数据存储写入一条segment记录的过程叫作pubulish
2. 这些记录又一个标志位used，代表是否可查
3. 实时摄取的segment会在被publish前就可以查询
******* 深度存储
segment一旦构造完成，立刻刷入深度存储中，然后写入meta存储
******* 支持可查
****** 查看segment
You can inspect the state of currently active segments using the Druid SQL sys.segments table. It includes the following flags:

is_published: True if segment metadata has been published to the metadata store and used is true.
is_available: True if the segment is currently available for querying, either on a realtime task or Historical process.
is_realtime: True if the segment is only available on realtime tasks. For datasources that use realtime ingestion, this will generally start off true and then become false as the segment is published and handed off.
is_overshadowed: True if the segment is published (with used set to true) and is fully overshadowed by some other published segments. Generally this is a transient state, and segments in this state will soon have their used flag automatically set to false.
**** segment的结构
1. 列式存储结构
2. ts列和metrics列
3. 维度列
   1. 值到id的对应关系字典
   2. 位图，对于某个值，标识哪些行包含它 最坏情况下，空间复杂度位行数*当前列可能的取值。为了减轻存储负担  可以用roaring bitmap compression.
   3. 将值替换位id的数据列表
***** why
In other words, queries that solely aggregate metrics based on filters do not need to touch the list of dimension values stored in (3)
**** files about segment
1. 00000.smoosh： 包含了所有列的文件和一个索引文件。放在一起以减少文件描述符的消耗
2. factory.json
3. meta.smoosh
4. version.bin
**** 列格式
1. jackson序列化的列描述符
2. 二进制的列值表
*** metadata
**** druid.metadata.storage.tables.segments表
1. 存储所有的used segment
2. coordinator使用这个表来决定可查询的segment
3. 主要的功能列有两列，剩下的主要用于indexing
   1. used： 标识segment是否应该可用
   2. payload ：一个json串，存储这个segment的元数据
**** rule table
segment的分配规则
**** config table
配置表，运行时修改配置的入口
**** Task-related tables
用于overload和middlemanager来保存任务信息
**** Audit table
历史审计表
*** lifeCycle
用来管理需要start和stop的对象
**** stages
***** 1. init
目前，这个阶段专门用于log4j初始化，因为几乎所有东西都需要日志记录，并且应该在最后关闭日志记录。任何一种bootstrapping对象，如果它提供了在几乎所有其他生命周期对象之前应该初始化的东西，那么它也可以属于这里(如果它在启动或停止期间不需要日志记录的话)。
***** 2. normal 
默认的，除了任何形式的服务器或服务声明之外，大多数对象在这个级别注册可能最有意义。
***** 3. server
这个生命周期阶段适用于所有的“服务器”对象，例如:org.apache.druid.server.initialization.jetty.JettyServerModule，但是任何类型的“服务器”，如果它希望大部分(或一些特定的)生命周期对象在它开始时被初始化，并且在它停止时仍然可用，那么它可以逻辑地生活在这个阶段
***** 4. ANNOUNCEMENTS
用来发布服务节点位置的对象属于这个阶段
*** 注册方式
*** router
web 控制台由router进程维护
*** storage
[[https://blog.bcmeng.com/post/druid-storage.html][Ref]]
**** segment存储
***** segment的实际存储文件有3个
****** 1. version.bin
4byte的二进制文件记录了segment的内部版本号
0000 0009 当前为v9
****** 2. meta.smooth 
一个列式存储文件
#+BEGIN_SRC sh
  #版本号,该文件所能存储的最大值(2G),smooth文件数
  v1,2147483647,1
  # 列名,文件名,起始偏移量,结束偏移量
  __time,0,0,154
  city,0,306,577
  gmv,0,154,306
  index.drd,0,841,956
  metadata.drd,0,956,1175
  sex,0,577,841
#+END_SRC
****** 


* 算法
** bitmap
[[http://hbasefly.com/2018/06/19/timeseries-database-8/][hbaseFly]]
* utils
** lifecycle
** NativeIO
ruid依赖linux的页缓存来缓存segment，但这两种情况的segment是不应该对页缓存产生大的占用的
1. 从deep storage 拉取segment
2. 只是为了rebalence segment而分配来的segment
优化： 使用sync_file_range来代替fsync来加速
* 用到的工具
** jackson
[[https://developer.ibm.com/zh/articles/jackson-advanced-application/][ibm-ref]]
* develop
** start point
数据存在segment的列中,可以从Column.java(在#5957中重命名为ColumnHolder.java)和继承它的类入手来了解存储格式.
** segment creation
1. IncrementalIndex.java 摄取数据
2. IndexMerger.java 创建segment
** storage engine
segment使用IndexIO.java映射内存,并为查询提供数据
** query engine
query逻辑去Query* 类看. 可以从QueryResouse.java下手
** coordination
1. historical的协调逻辑从DruidCoordinator.java下手
2. 实时摄取的协调逻辑从OverloadResource.java下手
** real-time ingestion
1. druid使用FirehoseFactory.java类来加载数据
2. hand-off 逻辑 RealtimePlumber.java
** hadoop-based batch ingestion
1. 决定创建多少segment  HadoopDruidDetermineConfigurationJob.java
2. 创建segment HadoopDruidIndexerJob.java
* paper
** 实时节点
*** 数据流动的4个阶段
1. ingest
2. persist
3. merge -> segment
4. hand off
*** 窗口
最小化丢数据的风险
1. 在窗口末尾去merge所有的persist index并执行hand off
2. 一旦这个窗口内的数据在historical节点可查了,刷出窗口内的所有信息,并撤销当前节点对这个窗口的server
** message bus
1. 用作消息事件的缓存,以便在failure和recover场景下正确消费消息
2. 统一的数据来源,多个消费节点可以冗余消费提升灾备能力或者分区消费提升消费速度
** 历史节点
遵循shared-nothing architecture
1. historical 彼此不认识
2. 只知道load, drop, server immutable segment
3. 和实时节点一样,通过zk发布服务状态和服务的数据
*** 分层
对historical分组,加载不同要求的segment.可以用来做冷热分离,配合规则
** 代理(broker)节点
理解zk上的segment信息,代理收到的查询,到正确的节点上收集数据,并汇总出最终结果.响应查询
*** LRU
用来缓存从历史节点查询指定segment的结果,不缓存从实时节点来的数据
** 协调节点
** mysql
1. 存需要由historical 节点接管的segment
* dump segments
2. 存规则,关于segment怎么创建? 怎么销毁? 怎么保存副本?[
[https://xixuebin.github.io/2019-04-01-092732-ch.html][使用DruidAPI dump Druid数据]]
* druid中的zk
** zk的用处
1. 协调节点和Overlord节点选主
2. 历史节点发布segement
3. 协调节点分配管理（load、drop）历史节点上的segment
4. Overlord和MiddleManager的任务管理
** 路径
- coordinator选主,overlord同理
#+begin_src bash
  [zk: localhost:2181(CONNECTED) 8] get /druid/cluster/coordinator/_COORDINATOR/_c_da02aa4f-2450-4616-923e-def24f6e0ba9-latch-0000000081


  # http://10.240.3.161:8081

#+end_src

- 历史节点和实时节点的服务发现，此处节点为临时节点
#+begin_src bash
  [zk: localhost:2181(CONNECTED) 35] ls /druid/cluster/announcements

  # [datanode01:8083, managernode01:8100, managernode01:8101, managernode01:8102, writenode01:8101, writenode02:8083, writenode02:8100, writenode02:8101, writenode02:8102]

#+end_src

- 历史节点加载的segment,此处历史节点为永久节点，其上加载的segment为临时节点
#+begin_src bash
[zk: localhost:2181(CONNECTED) 25] ls /druid/cluster/segments/datanode01:8083
# _segment_identifier_
#+end_src

- segment分配
#+begin_src bash
  # 要求datanode01:8083去load或者drop的segment
  [zk: localhost:2181(CONNECTED) 40] ls /druid/cluster/loadQueue/datanode01:8083/_identifier_segment

#+end_src
* Storage design 
** Indexing and handoff
indexing 是从数据创建segment， handoff是发布segment让历史节点提供查询
1. 一个indexing task构建一个segment的时候，segment应当有其唯一标识
   1. 在append模式下，唯一标示是通过overlord的 allocate 接口完成的
   2. 在overwrite模式下，是通过在interval上加锁并创建带有新版本号的segments
2. 如果indexing task是一个实时任务，则构建中的segment立刻可查，但是并未发布
3. 写好了一个segment后，indexing task 在元数据存储上发布该segment，并推入deep storage
4. 如果是实话任务，indexing task等待某个historical 节点加载segment并提供服务之后在删除本地的segment
*** coordinator和historical node
1. 协调节点周期的观察元数据中是否有新的segment被published
2. 当协调节点发现了published并used的segment，但是unavailable。其选择一个historical 节点加载该segment
3. 历史节点加载segment并提供服务
4. 等待handoff的实时任务结束
** Segment identifiers
由4部分组成
1. datasource name
2. Time interval
3. version number
4. partition number
** Segment versiong
用来支持batch-mode overwriting。当高版本的segment全部加载完成后，查询才会被导向高版本的segment
** Segment lifecycle
segment的生命周期设计3个地方
1. Metadata store ：segment写入metadata store的动作叫publish。used字段标示该segment是否可查
2. Deep stoarage ： 在segment publish之前将该segment推入Deep storage
3. 查询可用性 ： segment在历史节点或者实时节点上以供查询
** Availability and consistency
druid的摄取和查询在结构上分离，所以考察可用性和一致性的时候要分开看
*** ingestion
druid摄取是pull-based，提供事务保障。
**** 对于从kafka这类流式摄取
druid在publish segment的到元数据的同一个事务中同时存一个流offset。如果某个摄取任务失败，那么它已经摄取的部分数据将会被丢弃，
druid开始一个新的摄取任务，这个摄取任务从最后一次成功提交的segment记录的offset继续摄取
**** 基于hadoop的批量摄取
所有的segment元信息在同一个事务中发布
**** 本地批量摄取
1. parallel mode: 在子任务完成后publish所有的segment到metastore
2. simple mode： 所有的segment的元信息一起提交
*** query
broker负责在查询的时候来保证查询涉及的segments的一致性
1. atomic replacement 每个time chunk单独的完成原子替换
* 源码
** AppenderatorDriverRealtimeIndexTask
* 工作
1. checkpoint
2. compact
3. 资源管理(内存、线程)

