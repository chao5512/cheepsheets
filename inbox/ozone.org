#+title: ozone
* create key
** RpcClient
1. 校验，非空检查
2. 构造OmKeyArgs
3. 随机一个request uuid
4. 使用 OzoneManagerProtocol 打开一个OpenKeySession
5. createOutputStream(session)

* ReplicationManager

** 维护一个container的replica正确的冗余
只有container关闭的，我们才会对其replica进行维护
*** 先整理container，能关闭则关闭，关闭操作就是本次处理该container的唯一操作
1. scm上的container状态是open的，汇报上来的container处于这几种状态则关闭(更改scm上container的状态为CLOSING，向所有副本发送关闭命令)
   - 副本不是open的
   - 副本个数不对
   - 副本分布不对
2. scm上的container状态是CLOSING的，直接向所有副本发送关闭命令
3. scm上的container状态是QUASI_CLOSED,并且汇报上来的一半以上的非同源副本都准备好了关闭
*** 整理正在执行命令的副本的状态
把完成的或超时的命令从inflight集合中去掉
1. inflightRelication： 这是用来追踪副本增加命令的
2. inflightDeletion：这是用来追踪副本删除命令的
*** 开始检查副本，增删以保证正确冗余
1. container状态至少为CLOSED，且副本状态一致，视为健康，不做处理
2. 检查是否需要补充副本,两种状态需要补充
   1. 不符合防止策略
   2. 副本数少于冗余因子
3. 检查是否需要删除副本，1种状态可能需要删除
   1. 副本过剩

*** 如何补充副本？
分3部分
**** 1. 选择模版副本列表
状态为CLOSED和QUASI_CLOSED状态，且没有正在执行删除副本命令的副本所在dn，组成模版副本dn列表

**** 2. 选择增加副本的dn节点
排除已经持有该container和正在补充该container的dn

**** 3. 向选出来的dn发送副本增加命令
1. 只有在副本数量缺失，或者按1-2的补充计划后，副本的分布能够得到改善的时候，才会真正的发送命令
2. 发送了副本补充命令后在需要使用inflightReplication追踪该命令

*** 如何删除副本？

**** 选择需要删除的副本
1. 不能被删除的：每个同源(拥有相同的originID)的副本至少要保留一个
2. 需要被删除的
   1. 状态与container状态不一致的，如果此时副本数是过剩的，直接发送删除副本命令
   2. 删除原则：副本过剩的时候，如果已经是不正确的副本分布或者删除一个replica并不能使原本正确的分布变得不正确，总之就是删除一个副本不会使分布变得更坏



** container的状态转换
*** 引起container的状态转换事件
**** 1. CLOSE_CONTAINER事件
***** 触发close的事件的情况如下
- node failure
- volume failure
- volume out of space
- node out of space
***** close事件的主要逻辑
1. 改变scm上的container状态为CLOSING
2. 通知所有该container所在的dn转换该container为CLOSING

* scrub
负责数据的清理工作，当数据损坏时，及时的触发container-replication任务

** 数据检查

*** 元数据检查

**** 必要准备
1. container controller
2. 可配的检查周期

*** 数据检查
按volume划分检查任务

** 限流实现


** 磁盘管理

*** 磁盘检查

*** 磁盘选择

** container状态，不同于container-replica的状态

* 副本均衡

* 源码
** 前置配置
- 1. 可以在命令行或者启动脚本中配置ozone的配置文件
#+begin_src sh
[process of ozone] -conf xxx/ozone-default.xml
#+end_src


- 2. 在配置中规划好了所有进程的数据目录,可以降级到同一个目录下(方便配置,测试用)
  #+begin_src xml
    <property>
        <name>ozone.metadata.dirs</name>
        <value>rootdir/ozone/metaDir</value>
        <tag>OZONE, OM, SCM, CONTAINER, STORAGE, REQUIRED</tag>
        <description>
          This setting is the fallback location for SCM, OM, Recon and DataNodes
          to store their metadata. This setting may be used only in test/PoC
          clusters to simplify configuration.

          For production clusters or any time you care about performance, it is
          recommended that ozone.om.db.dirs, ozone.scm.db.dirs and
          dfs.container.ratis.datanode.storage.dir be configured separately.
        </description>
      </property>

  #+end_src
** SCM
*** init
在配置的scm.db.dirs中生成集群版本信息,生成scm节点的id信息
#+begin_src shell
  ozone scm --init

  #SCM initialization succeeded. Current cluster id for sd=/rootdir/ozone/metaDir/scm;cid=CID-efcf1bf1-a78b-4d9d-a99d-d80ecb768602;layoutVersion=0

  tree pathOfSCM

  #scm
  #└── current
  #    └── VERSION

  cat metaDir/scm/current/VERSION

  #Thu Oct 14 17:36:38 CST 2021
  nodeType=SCM
  scmUuid=043d1151-14a4-4e76-a667-267a24e3b99f
  clusterID=CID-efcf1bf1-a78b-4d9d-a99d-d80ecb768602
  cTime=1634204198060
  layoutVersion=0
#+end_src

*** ozone scm
启动scm

**** (optional)启动scm的web服务
相关配置
#+begin_src xml
    <property>
      <name>ozone.scm.http-address</name>
      <value>0.0.0.0:9876</value>
      <tag>OZONE, MANAGEMENT</tag>
      <description>
        The address and the base port where the SCM web ui will listen on.

        If the port is 0 then the server will start on a free port.
      </description>
    </property>
    <property>
      <name>ozone.scm.http-bind-host</name>
      <value>0.0.0.0</value>
      <tag>OZONE, MANAGEMENT</tag>
      <description>
        The actual address the SCM web server will bind to. If this
        optional address is set, it overrides only the hostname portion of
        ozone.scm.http-address.
      </description>
    </property>
    <property>
      <name>ozone.scm.http.enabled</name>
      <value>true</value>
      <tag>OZONE, MANAGEMENT</tag>
      <description>
        Property to enable or disable SCM web ui.
      </description>
    </property>
    <property>
      <name>ozone.scm.https-address</name>
      <value>0.0.0.0:9877</value>
      <tag>OZONE, MANAGEMENT</tag>
      <description>
        The address and the base port where the SCM web UI will listen
        on using HTTPS.

        If the port is 0 then the server will start on a free port.
      </description>
    </property>
    <property>
      <name>ozone.scm.https-bind-host</name>
      <value>0.0.0.0</value>
      <tag>OZONE, MANAGEMENT</tag>
      <description>
        The actual address the SCM web server will bind to using HTTPS.
        If this optional address is set, it overrides only the hostname portion of
        ozone.scm.https-address.
      </description>
    </property>

#+end_src

***** TODO scm web 页面 可以获得的信息

**** 启动后scm目录下的文件结构
#+begin_src sh
  metaDir
  ├── db.checkpoints
  ├── scm
  │   └── current
  │       └── VERSION
  └── scm.db  //用rokcsdb来作为scm的持久化存储
      ├── 000024.log
      ├── CURRENT
      ├── IDENTITY
      ├── LOCK
      ├── LOG
      ├── LOG.old.1634205202869713
      ├── LOG.old.1634205228566554
      ├── LOG.old.1634205442027664
      ├── LOG.old.1634205709142623
      ├── MANIFEST-000023
      ├── OPTIONS-000023
      └── OPTIONS-000026

#+end_src

**** 启动不同的服务
对不同的protocol,在不同的端口上启动不同的服务,在log中记录
#+begin_src text
	todo : 可以看这几个protocol面向的场景
  INFO   StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
  INFO   ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
  INFO   ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
#+end_src

*** allocateBlock
分配一个指定大小的block
**** 逻辑概述
1. 找到所有打开状态的pipeline
2. 如果没有打开状态的pipeline,则创建一个
3. 在打开的pipeline可用的container上分配一个block
*** createPipeline
1. 根据策略和需求选择一些datanode
2. 可以在这里做一些预操作,例如构建ratisPipeline:向每个dn发送cmd,在dn之间构建起这个pipeline
   1. 如果这里简单使用hdfs的pipeline,则可以直接返回dn列表构建起的pipeline即可

** ozone datanode

*** 配置存储目录
不配置会在/tmp目录下生成临时文件
#+begin_src xml
  <property>
      <name>hdds.datanode.dir</name>
      <value>/xxx/ozone/volumes</value>
      <tag>OZONE, CONTAINER, STORAGE, MANAGEMENT</tag>
      <description>Determines where on the local filesystem HDDS data will be
        stored. Defaults to dfs.datanode.data.dir if not specified.
        The directories should be tagged with corresponding storage types
        ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for storage policies. The default
        storage type will be DISK if the directory does not have a storage type
        tagged explicitly.
      </description>
  </property>

#+end_src

目录结构
#+begin_src shell
  tree path
  #── volumes
  #    ├── hdds
  #    └── scmUsed


  cat scmUsed

  #8192 1634210195271% 
#+end_src

*** container存储结构
#+begin_src text
  └── volumes
      ├── hdds
      │   ├── 043d1151-14a4-4e76-a667-267a24e3b99f    //scm uuid
      │   │   └── current
      │   │       └── containerDir0
      │   │           ├── 3
      │   │           │   ├── chunks
      │   │           │   │   ├── 107104110352138242.block
      │   │           │   │   ├── 107104143830548485.block
      │   │           │   │   ├── 107104264322678795.block
      │   │           │   │   ├── 107104346305069070.block
      │   │           │   │   ├── 107104351582093329.block
      │   │           │   │   ├── 107104370581897236.block
      │   │           │   │   └── 107104376491409431.block
      │   │           │   └── metadata
      │   │           │       ├── 3.container
      │   │           │       └── 3-dn-container.db
      │   │           │           ├── 000004.sst
      │   │           │           ├── 000006.log
      │   │           │           ├── CURRENT
      │   │           │           ├── IDENTITY
      │   │           │           ├── LOCK
      │   │           │           ├── LOG
      │   │           │           ├── LOG.old.1634282567964969
      │   │           │           ├── MANIFEST-000005
      │   │           │           ├── OPTIONS-000005
      │   │           │           └── OPTIONS-000008
#+end_src

*** 使用rocksdb作为元数据存储,每个元数据一个库
每个container的元数据存储在自己的rocksdb中

**** 使用jmx获得每个container中rocksdb的统计信息
1. 将ozone.metastore.rocksdb.statistics打开,这会带来5%-10%的性能损耗
   #+begin_src xml
     <property>
         <name>ozone.metastore.rocksdb.statistics</name>
         <value>ALL</value>
         <tag>OZONE, OM, SCM, STORAGE, PERFORMANCE</tag>
         <description>
           The statistics level of the rocksdb store. If you use any value from
           org.rocksdb.StatsLevel (eg. ALL or EXCEPT_DETAILED_TIMERS), the rocksdb
           statistics will be exposed over JMX bean with the choosed setting. Set
           it to OFF to not initialize rocksdb statistics at all. Please note that
           collection of statistics could have 5-10% performance penalty.
           Check the rocksdb documentation for more details.
         </description>
       </property>

   #+end_src
2. 在datanode启动的server端口上使用jmx命令
   #+begin_src text
     http://localhost:9882/jmx

     # 获得如下信息
     {
         "name" : "Hadoop:service=Ozone,name=RocksDbStore,dbName=5-dn-container.db",
         "modelerType" : "",
         "BLOCK_CACHE_MISS" : 6,
         "BLOCK_CACHE_HIT" : 29,
         "BLOCK_CACHE_ADD" : 6,
         "BLOCK_CACHE_ADD_FAILURES" : 0,
         ....
     },
     {
         "name" : "Hadoop:service=Ozone,name=RocksDbStore,dbName=4-dn-container.db",
         "modelerType" : "",
         "BLOCK_CACHE_MISS" : 6,
         "BLOCK_CACHE_HIT" : 29,
         "BLOCK_CACHE_ADD" : 6,
         "BLOCK_CACHE_ADD_FAILURES" : 0,
         "BLOCK_CACHE_INDEX_MISS" : 0,
         ...
     },
     ....
   #+end_src
*** (optional) 启动datanode的web服务
配置
#+begin_src xml
    <property>
      <name>hdds.datanode.http-address</name>
      <value>localhost:9882</value>
      <tag>HDDS, MANAGEMENT</tag>
      <description>
        The address and the base port where the Datanode web ui will listen on.
        If the port is 0 then the server will start on a free port.
      </description>
    </property>
    <property>
      <name>hdds.datanode.http-bind-host</name>
      <value>localhost</value>
      <tag>HDDS, MANAGEMENT</tag>
      <description>
        The actual address the Datanode web server will bind to. If this
        optional address is set, it overrides only the hostname portion of
        hdds.datanode.http-address.
      </description>
    </property>

#+end_src

***** jetty 依赖找不到
ozone NoClassDefFoundError: org/eclipse/jetty/server/ConnectionFactory
****** 原因
#+begin_src xml
  <!--注释掉一下几个scope为test的属性-->
  <!-- 在 container-service的pom中-->
      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdds-hadoop-dependency-test</artifactId>
        <!--<scope>test</scope>-->
      </dependency>
      <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <!--<scope>test</scope>-->
        <type>test-jar</type>
      </dependency>

      <!--在最外层pom中-->
      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-hdds-hadoop-dependency-test</artifactId>
          <version>${hdds.version}</version>
          <!--<scope>test</scope>-->
        </dependency>
#+end_src
**** 与scm交互
使用 ScmDatanodeProtocl协议来与scm交互

#+begin_src text
  # 生成一个新的数据路径
  └── volumes
      ├── hdds
      │   ├── 043d1151-14a4-4e76-a667-267a24e3b99f
      │   └── VERSION
      └── scmUsed

  # 生成新的ratis组
  ├── ratis
  │   │   └── ae6096dd-84c9-4fae-8674-56c060fdc170
  │   │       ├── current
  │   │       │   ├── log_inprogress_0
  │   │       │   ├── raft-meta
  │   │       │   └── raft-meta.conf
  │   │       ├── in_use.lock
  │   │       └── sm


#+end_src
*** 删掉了datanode.id文件后
启动报错
#+begin_src java
  // VERSION 文件中记录的datanodeid和从datanode.id中读到的id不一致
  throw new InconsistentStorageStateException("Mismatched " +
            "DatanodeUUIDs. Version File : " + versionFile + " has datanodeUuid: "
            + datanodeID + " and Datanode has datanodeUuid: " + datanodeUuid);
#+end_src

当删除了datanode.id后,datanode启动时找不到id文件,则使用UUID.randomUUID将此datanode作为一个新的数据节点启动
** ozone om

*** init
在配置中读取scm配置的ScmBlockLocationProtocol协议在scm上的服务,然后通过该协议问scm信息,包括集群id和scm id

然后在配置的本地元数据目录下写下该信息
#+begin_src text
  │   ├── om
  │   │   └── current
  │   │       └── VERSION


  # cat VERSION

  #Fri Oct 15 10:11:21 CST 2021
  nodeType=OM
  scmUuid=043d1151-14a4-4e76-a667-267a24e3b99f
  clusterID=CID-efcf1bf1-a78b-4d9d-a99d-d80ecb768602
  cTime=1634263881350
  omUuid=e8e5ca94-7f88-48e7-90ae-ee56493bdf86
  layoutVersion=0

#+end_src

*** 启动om
会在配置的om db目录下防止om rocksdb的数据

**** om系统表,结构如下
   * |----------------------------------------------------------------------|
   * |  Column Family     |        VALUE                                    |
   * |----------------------------------------------------------------------|
   * | userTable          |     /user->UserVolumeInfo                       |
   * |----------------------------------------------------------------------|
   * | volumeTable        |     /volume->VolumeInfo                         |
   * |----------------------------------------------------------------------|
   * | bucketTable        |     /volume/bucket-> BucketInfo                 |
   * |----------------------------------------------------------------------|
   * | keyTable           | /volumeName/bucketName/keyName->KeyInfo         |
   * |----------------------------------------------------------------------|
   * | deletedTable       | /volumeName/bucketName/keyName->RepeatedKeyInfo |
   * |----------------------------------------------------------------------|
   * | openKey            | /volumeName/bucketName/keyName/id->KeyInfo      |
   * |----------------------------------------------------------------------|
   * | s3SecretTable      | s3g_access_key_id -> s3Secret                   |
   * |----------------------------------------------------------------------|
   * | dTokenTable        | s3g_access_key_id -> s3Secret                   |
   * |----------------------------------------------------------------------|
   * | prefixInfoTable    | prefix -> PrefixInfo                            |
   * |----------------------------------------------------------------------|
   * |  multipartInfoTable| /volumeName/bucketName/keyName/uploadId ->...   |
   * |----------------------------------------------------------------------|
   * |----------------------------------------------------------------------|
   * |  transactionInfoTable | #TRANSACTIONINFO -> OMTransactionInfo        |
   * |----------------------------------------------------------------------|

*** om 用于元数据管理的table
   * OM DB stores metadata as KV pairs in different column families.
   * <p>
   * OM DB Schema:
   * |----------------------------------------------------------------------|
   * |  Column Family     |        VALUE                                    |
   * |----------------------------------------------------------------------|
   * | userTable          |     /user->UserVolumeInfo                       |
   * |----------------------------------------------------------------------|
   * | volumeTable        |     /volume->VolumeInfo                         |
   * |----------------------------------------------------------------------|
   * | bucketTable        |     /volume/bucket-> BucketInfo                 |
   * |----------------------------------------------------------------------|
   * | keyTable           | /volumeName/bucketName/keyName->KeyInfo         |
   * |----------------------------------------------------------------------|
   * | deletedTable       | /volumeName/bucketName/keyName->RepeatedKeyInfo |
   * |----------------------------------------------------------------------|
   * | openKey            | /volumeName/bucketName/keyName/id->KeyInfo      |
   * |----------------------------------------------------------------------|
   * | s3SecretTable      | s3g_access_key_id -> s3Secret                   |
   * |----------------------------------------------------------------------|
   * | dTokenTable        | s3g_access_key_id -> s3Secret                   |
   * |----------------------------------------------------------------------|
   * | prefixInfoTable    | prefix -> PrefixInfo                            |
   * |----------------------------------------------------------------------|
   * |  multipartInfoTable| /volumeName/bucketName/keyName/uploadId ->...   |
   * |----------------------------------------------------------------------|
   * |----------------------------------------------------------------------|
   * |  transactionInfoTable | #TRANSACTIONINFO -> OMTransactionInfo        |
   * |----------------------------------------------------------------------|
   *
 

** ozone 写入数据流程

*** ozone-client与hdds-container-services的交互
通过DatanodeClientProtocol.proto 这个协议,将数据组织成一个个chunk,封装为writeChunk(ContainerCommandRequestProto)
将数据内容和crc一同发往指定dn的指定container的指定block

**** 在datanode上如何将接收到的数据持久化存储起来的?

*** 数据节点如何创建一个container

1. 创建一个container需要数据节点之外的节点分配好container id和pipeline.
2. 数据节点的container实现可以不一样,container中chunk的布局可以每个数据节点单独配置
3. container的创建过程是排他的

** ozone pipeline创建
1. scm发送创建pipeline的命令给所有的dn后(只是发布了创建pipeline事件)
2. scm添加该pipeline到pipeline表中,并缓存在内存中
3. scm计算下是否有打开的pipeline是和本次创建的pipeline拥有相同的dn集合,这是不应该的,如果dn相同,则应该复用pipeline

** ozone fs设计

*** 分离式namespce

**** 好处
***** 减少namespce元数据
多个path共用一个前缀,封底估算:在这样的场景下,文件名大概占32bytes,整个path占256bytes,一亿个文件就是
全路径格式消耗:256G
公用前缀消耗:32G
节省了200多G,这对于可能需要做缓存的数据来说,提升非常大
***** 可以将dir table全表缓存在内存中
***** 对rename等操作来说,极大的减少了成本

** ratis

*** Common

**** DataQueue
相比与原生的queue的队列长度限制,DataQueue多了队列内元素占用空间的限制

*** ServerApi

**** LogAppender
用来在leader上发送raftlog到followers

**** RaftLog
rtis提供两类raftlog
1. MemoryRaftLog: 所有的raftlog都在内存中,应只用于test
2. Segmented RaftLog: 存储在磁盘上

***** SegmentedRaftLog
1. log追加到磁盘上
2. 按segments组织所有的Raftlog,每个segment尽可能8M左右
3. 一个entry大于8M的情况下,这个segment只存这个entry
4. 分为Closed segement 和 open segment

5. 从open到closed的reson: 到达大小限制,log term更迭
   
6. closed segment不可追加但是可以切分
7. 可以有多个closed segment,但只能有一个opensegment

****** 实现细节
设置一个io worker其包含一个task队列,所有想要追加的raftlog作为task添加至队列
思考:入队速度大于出队速度怎么办?
首先:入不了队就一直入,每一秒尝试一次,入队操作持有raftlock,背压上推

* 集群搭建
1.2.1后支持om HA,这里记录1.2.1的部署
** 规划
3台机器,3个om,3个scm,3个datanode
** 配置
#+begin_src xml
  <configuration>
      <property>
          <name>ozone.om.address</name>
          <value>localhost</value>
          <tag>OM, REQUIRED</tag>
          <description>
        The address of the Ozone OM service. This allows clients to discover
        the address of the OM.
      </description>
      </property>
      <property>
          <name>ozone.metadata.dirs</name>
          <value>/data/br/data/ozone/meta</value>
          <tag>OZONE, OM, SCM, CONTAINER, STORAGE, REQUIRED</tag>
          <description>
        This setting is the fallback location for SCM, OM, Recon and DataNodes
        to store their metadata. This setting may be used only in test/PoC
        clusters to simplify configuration.

        For production clusters or any time you care about performance, it is
        recommended that ozone.om.db.dirs, ozone.scm.db.dirs and
        dfs.container.ratis.datanode.storage.dir be configured separately.
      </description>
      <!--这里配置datanode的ratis保存位置,不然会和scm的ratis冲突-->
      </property>
          <property>
          <name>dfs.container.ratis.datanode.storage.dir</name>
          <value>/data/br/data/ozone/meta/dnratis</value>
      </property>
      <property>
          <name>ozone.scm.client.address</name>
          <value>localhost</value>
          <tag>OZONE, SCM, REQUIRED</tag>
          <description>
        The address of the Ozone SCM client service. This is a required setting.

        It is a string in the host:port format. The port number is optional
        and defaults to 9860.
      </description>
      </property>
      <property>
          <name>ozone.scm.names</name>
          <value>localhost</value>
          <tag>OZONE, REQUIRED</tag>
          <description>
        The value of this property is a set of DNS | DNS:PORT | IP
        Address | IP:PORT. Written as a comma separated string. e.g. scm1,
        scm2:8020, 7.7.7.7:7777.
        This property allows datanodes to discover where SCM is, so that
        datanodes can send heartbeat to SCM.
      </description>
      </property>

  <property>
     <name>ozone.scm.ratis.enable</name>
     <value>true</value>
  </property>
  <property>
     <name>ozone.scm.service.ids</name>
     <value>cluster1</value>
  </property>

  <property>
     <name>ozone.scm.nodes.cluster1</name>
     <value>scm1,scm2,scm3</value>
  </property>

  <property>
     <name>ozone.scm.address.cluster1.scm1</name>
     <value>10.241.150.10</value>
  </property>
  <property>
     <name>ozone.scm.address.cluster1.scm2</name>
     <value>10.241.150.11</value>
  </property>
  <property>
     <name>ozone.scm.address.cluster1.scm3</name>
     <value>10.241.150.12</value>
  </property>


  <property>
     <name>ozone.om.ratis.enable</name>
     <value>true</value>
  </property>
  <property>
    <name>ozone.om.service.ids</name>
    <value>cluster1</value>
  </property

  <property>
     <name>ozone.om.nodes.cluster1</name>
     <value>om1,om2,om3</value>
  </property>

  <property>
     <name>ozone.om.address.cluster1.om1</name>
     <value>10.241.150.10</value>
  </property>
  <property>
     <name>ozone.om.address.cluster1.om2</name>
     <value>10.241.150.11</value>
  </property>
  <property>
     <name>ozone.om.address.cluster1.om3</name>
     <value>10.241.150.12</value>
  </property>
  <!--datanode volume 位置-->
  <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:///data/br/data/ozone/data</value>
  </property>
  </configuration>
#+end_src
** sh
*** scm HA
1. 在第一台机器上 ozone scm --init

2. 在第一台机器上启动scm : ozone --daemon start scm
   
3. 在第二,第三台机器上 ozone scm --bootstrap 链接第一台scm,初始化本地一些必要的目录和结构

4. 在第二,第三台机器上启动scm : ozone --daemon start scm
**** 检查
检查的主要任务是确认我们搭建的scm ha集群确实是在一个存活组里,而不是启动了3个scm
1. 可以查看3个scm的httpserver(默认端口9876),观察Cluster id是否一致
2. 可以去各个scm的元数据下,观察Cluster id是否一致
*** OM HA
1. 在第一台机器上 ozone om --init

2. 在第一台机器上启动om, ozone --daemon start om

3. 在第二,第三台机器上 ozone om --bootstrap 链接第一台om,并生成本地必要的元数据目录

4. 在第二,第三台机器上启动om, ozone --daemon start om
**** 检查
查看服务角色
#+begin_src sh
  # 这个cluster1是配置中配置的
  ./bin/ozone admin om getserviceroles -id=cluster1


  #result
  om3 : FOLLOWER (br-test-12)
  om2 : FOLLOWER (br-test-11)
  om1 : LEADER (br-test-10)
#+end_src
*** data node
datanode需要注意元数据和volume中的Version信息
1. 启动datanode ozone --daemon start datanode
**** 检查
可以在scm的HttpServer上看到 inserver状态的dn个数
** 客户端配置
*** 依赖
这里不同版本的hadoop客户端用不同的ozone兼容依赖
#+begin_src xml
  <dependency>
          <groupId>org.apache.ozone</groupId>
          <artifactId>hdds-client</artifactId>
          <version>1.2.1</version>
      </dependency>

      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-client</artifactId>
          <version>3.1.0</version>
      </dependency>

      <!-- https://mvnrepository.com/artifact/org.apache.ozone/ozone-filesystem-hadoop3 -->
      <dependency>
          <groupId>org.apache.ozone</groupId>
          <artifactId>ozone-filesystem-hadoop3</artifactId>
          <version>1.2.1</version>
      </dependency>
#+end_src
*** resource
复制core-site.xml 和 ozone-site.xml到resource中
**** core-site.xml
#+begin_src xml
  <configuration>
      <property>
          <name>fs.ofs.impl</name>
          <value>org.apache.hadoop.fs.ozone.RootedOzoneFileSystem</value>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>ofs://cluster1/</value>
      </property>
  </configuration>
#+end_src
**** ozone-site.xml
从ozone集群复制
*** code
#+begin_src java
  public static void main(String[] args) {
          try {
              // 配置连接地址
              Configuration conf = new Configuration();
              OzoneConfiguration ozoneConfiguration = new OzoneConfiguration();
              conf.addResource(ozoneConfiguration);

              FileSystem fs = FileSystem.get(conf);
              // 打开文件并读取输出
              Path hello = new Path("/volume/bucket/HISTORY.md");
              FSDataInputStream ins = fs.open(hello);
              int ch = ins.read();
              while (ch != -1) {
                  System.out.print((char)ch);
                  ch = ins.read();
              }
              System.out.println();
          } catch (IOException ioe) {
              ioe.printStackTrace();
          }
      }
#+end_src
** ozone fs cmd
#+begin_src shell
  # list
  ozone fs -ls -R ofs://cluster1/
#+end_src
** 配置明细
*** ratis相关
**** dfs.container.ratis.log.appender.queue
从leader向follower发送的raftlog的长度和大小的限制
1. dfs.container.ratis.log.appender.queue.byte-limit = 32M 
2. dfs.container.ratis.log.appender.queue.element-limit = 0 // (0表示队列长度不受限制)

**** dfs.container.ratis.log.queue
ratis log所有与io相关的操作都由固定的woker来做,io 操作被封装为task,排进这个队列(这种task在队列中排队的时间和执行时间可以被监控).由worker从队列中取任务做落盘追加.
1. queue.byte-limit  = 64M
2. dfs.container.ratis.log.queue.num-elements = 1024
*** dfs相关
**** dfs.container.ratis.enabled
作用于om
如果文件写入请求中没有指定副本类型和副本数,则此时该配置发挥作用.

* 问题排查
** 写入超时
创建pipeline时,某个dn加入ratis group时失败,why?
datanode 一直刷日志
#+begin_src text
  INFO org.apache.ratis.server.RaftServer$Division: 5f5c4efb-1055-4fbe-9175-4a9b426d31a6@group-A715AD653930-LeaderStateImpl send StartLeaderElectionRequest to follower:06fb1d2e-4b04-4d13-a027-fba46eb6f7f1 on term:4 because follower's priority:1 is higher than leader's:0 and follower's lastEntry index:2 catch up with leader's:2

#+end_src
* dfs 和 db
dfs偏离线
db偏实时
** 现有db局限性
1. 扩容
2. 数据孤岛
3. 实时和离线数据需要两部分资源
** HTAP
*** 标准
1. 弹性扩容
2. 业务无需妥协
   1. sql
   2. 分布式事务
   3. 复杂查询
3. 高可用
4. 支持高性能实时分析(为什么列式analysis?最快)
5. 实时olap分析不影响oltp
* something
[[https://monodraw.helftone.com/][mac uml]]

